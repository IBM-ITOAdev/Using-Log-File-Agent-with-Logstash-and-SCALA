##################################################
# Logstash for ITM LFA Termination
#
# v1.0 	12/29/14	Doug McClure
#
##################################################
# NOTES:
# 1. This configuration will work with logstash 1.4.2 or logstash 1.5.0 beta1 using the latest scala logstash output plugin. Contact Doug McClure for guidance on aquiring this output plugin.
# 2. The standard TCP input can be used to terminate ITM LFA connections. Running through the codec will ensure the EIF formatting is properly processed.
# 3. The standard TCP input with SSL configurations can be used to terminate ITM LFA connections sent using TLS/SSL encryption.
# 4. The default EIF Receiver port for SCALA is 5529 so be sure to choose a different port for logstash to listen to if on the same server.
# 5. After stripping away the EIF wrapper, the original log message/data is in the field named "text"
# 6. Be aware of any multiline log messages that might need to be dealt with here if you haven't fixed them in your content pack code in SCALA. The multiline filter works on the message field so you would need to replace the message field contents with the text field contents first, then run through the multiline filter to consolidate into one log message.  
# 7. If you're going to ship the original log message to an existing SCALA content pack (eg WebSphere or DB2 content pack), you'll need to replace the "message" field with the "text" field contents using a mutate filter. By default, the scala output will ship the host, path and message fields to SCALA's API.
# 8. Set your host and path as needed for aggregation and consolidation in your SCALA datasource. For example, assuming you're using an many LFAs across many different servers to send your WebSphere Application Server logs, this configuration would send all of those logs to one SCALA datasource named "All-WebSphere-Logs".  Use conditionals to create as many variations as needed to aggregrate and consolidate into datasources as needed (eg by application, by datacenter, by function, by server, etc.)
# 9. Set a "final" tag to indicate all processing is done and message is ready to ship to SCALA via output
# 10. If parsing & annotation will be done in your SCALA content pack, simply let the message exit the scala output by using a "final" tag. The output will automatically send host, path and message field to SCALA API.
# 11. If you are using a SCALA DSV content pack, simply define your fields below in the order expeced in your DSV properties file
#	Example field definition in scala_fields section below:
#	"SCAPI-STAT-DataAvail@SCAPI-STAT-DataAvail" => "@timestamp,Process,Resources,KPIs,OfMaxKPIs,NoLongerAnalyzed,AlarmsSent"
#
#####################################################

input {

    #using standard ITM LFA w/o SSL
	tcp {
		port => 5530
		codec => line { charset => "US-ASCII" }
		type => "LFA-EIF"
	} #end tcp		
		
	#using standard ITM LFA w/ TLS/SSL (details on configuring the LFA and logstash will be provided in another cookbook)
	#tcp {
	#	port => 5531
	#	ssl_enable => true
	#	ssl_cert => "/etc/pki/tls/myCA/server_crt.pem"
	#	ssl_key => "/etc/pki/tls/myCA/server_key.pem"
	#	ssl_cacert => "/etc/pki/tls/myCA/cacert.pem"
	#	ssl_verify => false
	#	type  => "LFA-SSL"
	#	codec => line { charset => "US-ASCII" }
	#} #end SSL input	
		
		
} # end input

###################################################3

filter {

if [type] == "LFA-EIF" {
	grok {
		match => ["message", "%{LFAMESSAGE}"]
		patterns_dir => ["/opt/logstash150b1/patterns"]
		add_tag => [ "LFA-EIF-Grokked" ]
	} #end grok
} #end main grok conditional

if "LFA-EIF-Grokked" in [tags] {
	mutate {
		replace => ["message","%{text}"]
		add_tag => ["LFA-EIF-Message-Replaced"]
	} # end mutate
} #end message replacement

if "LFA-EIF-Message-Replaced" in [tags] {
	mutate {
		replace => [ "host", "All-WebSphere-Logs", "path", "All-WebSphere-Logs"]
		add_tag => ["LFA-EIF-final"]
	} #end host/path mutate
} #end host/path setup

} # end filters

###################################################

output {

################################
# for debugging
################################

stdout {
    codec => rubydebug
    } #end stdout
	
##################################	
#for new logstash scala outuput 
##################################	
		
if "LFA-EIF-final" in [tags] {
	scala { 
		scala_url => "https://10.0.0.1:9987/Unity/DataCollector"
		scala_user => "unityadmin" 
		scala_password => "unityadmin" 
		batch_size => 500000
		idle_flush_time => 5 
		sequential_flush => false
		num_concurrent_writers => 5
		use_structured_api => false
		disk_cache_path => "/opt/logstash/cache/tmp_spool_dir_LFA-EIF"
		
		#when using DSV content packs, add your host@path field definitions here for CSV output
		scala_fields => {
		   
		} #end scala_fields
		date_format_string => "yyyy-MM-dd'T'HH:mm:ssX"
	} #end scala output
} #end scala output conditional
		
} # end output
